{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a420e558-4bc3-46f8-b1e1-764ad6c52b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01360974-8a7d-4822-bea5-066317393cd3",
   "metadata": {},
   "source": [
    "### Notebook 4 â€” Model Capability Profile Construction\n",
    "\n",
    "This notebook transforms benchmark-level achievement data into aggregated model-level capability profiles.\n",
    "\n",
    "It computes a composite quality score using performance, peer review status, and human comparison indicators, then aggregates these metrics by model.\n",
    "\n",
    "The resulting model profiles provide structured inputs for downstream routing and cost-performance optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea28dc2-79d1-4869-a523-80f2447017f1",
   "metadata": {},
   "source": [
    "### Configure AWS Environment and Data Access\n",
    "\n",
    "Initialize AWS session and define S3 paths for reading input data and writing processed outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "526457a5-312d-4ceb-ada6-772278291e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: s3://sagemaker-us-east-1-907086662522/llmachievements.csv\n"
     ]
    }
   ],
   "source": [
    "# --- S3 paths ---\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "raw_path = f\"s3://{bucket}/llmachievements.csv\"\n",
    "out_local = \"model_profiles.csv\"\n",
    "out_s3 = f\"s3://{bucket}/processed/model_profiles.csv\"\n",
    "\n",
    "print(\"Reading:\", raw_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65934e97-e9a4-42d1-b627-ecfb9da7cfa7",
   "metadata": {},
   "source": [
    "### Load Benchmark Achievement Data\n",
    "\n",
    "Read the benchmark achievement dataset from Amazon S3.\n",
    "\n",
    "This dataset contains model performance results across evaluation domains and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6b7deb7-369c-4f3e-a5bb-94021a06fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5522465f-da2d-4db1-8958-39b588a237a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = [\"Model\", \"Field\", \"Achievement\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Dataset missing required columns: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6726a7d-faa3-4826-8418-64f837a0cfa1",
   "metadata": {},
   "source": [
    "### Standardize and Clean Dataset Columns\n",
    "\n",
    "Normalize column names and data formats to ensure consistent processing.\n",
    "\n",
    "This step prepares the dataset for feature engineering and aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "126d0f02-a74e-4dc3-803c-f806ce46ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- basic cleaning ---\n",
    "# normalize column names\n",
    "df.columns = [c.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \") for c in df.columns]\n",
    "\n",
    "# handle the weird \"Peer-\\nreviewed?\" header if it exists\n",
    "peer_col = None\n",
    "for c in df.columns:\n",
    "    if \"Peer\" in c and \"review\" in c.lower():\n",
    "        peer_col = c\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554075a5-0a6b-4e74-9554-00b483b0b489",
   "metadata": {},
   "source": [
    "### Engineer Model Performance Indicators\n",
    "\n",
    "Create numeric indicators that capture important evaluation signals, including:\n",
    "\n",
    "- Peer review status  \n",
    "- Human performance comparison  \n",
    "- Normalized benchmark results  \n",
    "\n",
    "These features contribute to the model quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbc39cc1-cf62-43d8-b794-62bb66642489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize text fields\n",
    "for col in [\"Model\", \"Field\", \"Outperforms human avg?\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "# coerce numeric result fields if present\n",
    "for col in [\"Result\", \"Human result\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# normalize peer-reviewed to boolean-ish\n",
    "if peer_col:\n",
    "    df[peer_col] = df[peer_col].astype(str).str.strip().str.lower()\n",
    "    df[\"peer_reviewed_flag\"] = df[peer_col].isin([\"yes\", \"y\", \"true\", \"1\"])\n",
    "else:\n",
    "    df[\"peer_reviewed_flag\"] = False\n",
    "\n",
    "# normalize \"outperforms\" to boolean-ish\n",
    "if \"Outperforms human avg?\" in df.columns:\n",
    "    df[\"outperforms_flag\"] = df[\"Outperforms human avg?\"].astype(str).str.strip().str.lower().isin([\"yes\", \"y\", \"true\", \"1\"])\n",
    "else:\n",
    "    df[\"outperforms_flag\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da2734-6609-473c-9e73-6cc3292c5814",
   "metadata": {},
   "source": [
    "### Compute Composite Quality Score\n",
    "\n",
    "Calculate a weighted performance score that summarizes model capability across benchmark evaluations.\n",
    "\n",
    "The score combines multiple performance indicators into a single interpretable metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e90f894f-19f5-41ba-ab9f-5b687675fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build a \"quality score\" ---\n",
    "# Simple, explainable scoring:\n",
    "# +2 if outperforms human\n",
    "# +1 if peer-reviewed\n",
    "# + up to +2 from numeric Result (scaled to 0..2)\n",
    "# +0.5 if Human result exists (means comparable benchmark exists) : proxy for design doc\n",
    "score = np.zeros(len(df), dtype=float)\n",
    "\n",
    "score += 2.0 * df[\"outperforms_flag\"].astype(float)\n",
    "score += 1.0 * df[\"peer_reviewed_flag\"].astype(float)\n",
    "\n",
    "if \"Result\" in df.columns:\n",
    "    # normalize Result to 0..2 using percentile scaling (robust)\n",
    "    r = df[\"Result\"].copy()\n",
    "    r_min = np.nanpercentile(r, 5) if np.isfinite(r).any() else 0\n",
    "    r_max = np.nanpercentile(r, 95) if np.isfinite(r).any() else 100\n",
    "    denom = (r_max - r_min) if (r_max - r_min) != 0 else 1.0\n",
    "    r_norm = ((r - r_min) / denom).clip(0, 1)\n",
    "    score += 2.0 * r_norm.fillna(0)\n",
    "\n",
    "if \"Human result\" in df.columns:\n",
    "    score += 0.5 * df[\"Human result\"].notna().astype(float)\n",
    "\n",
    "df[\"quality_score\"] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402125e-9fd7-4061-9b00-2975da8ab2fd",
   "metadata": {},
   "source": [
    "### Aggregate Metrics at Model Level\n",
    "\n",
    "Group benchmark results by model and compute summary statistics, including:\n",
    "\n",
    "- Mean performance score  \n",
    "- Evaluation coverage  \n",
    "- Domain diversity  \n",
    "\n",
    "This produces one profile per model.\n",
    "\n",
    "### Assign Performance Tiers\n",
    "\n",
    "Classify models into relative performance tiers based on aggregated quality scores.\n",
    "\n",
    "This enables simplified comparison and routing decisions.\n",
    "\n",
    "#### Derive Domain Coverage Features\n",
    "\n",
    "Identify evaluation domains associated with each model to measure breadth of capability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "271cecb7-d7cb-4c09-8900-9626afc0eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model profiles preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>quality_score_mean</th>\n",
       "      <th>quality_score_max</th>\n",
       "      <th>achievements_count</th>\n",
       "      <th>domains_count</th>\n",
       "      <th>peer_reviewed_rate</th>\n",
       "      <th>outperforms_rate</th>\n",
       "      <th>domains_covered</th>\n",
       "      <th>quality_tier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gemini 3</td>\n",
       "      <td>4.476880</td>\n",
       "      <td>4.476880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Transcription</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Claude 3.6S</td>\n",
       "      <td>4.415792</td>\n",
       "      <td>4.415792</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>o3-mini-high</td>\n",
       "      <td>4.360643</td>\n",
       "      <td>4.360643</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Health reviews</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>o1</td>\n",
       "      <td>4.040036</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Maths, Medicine</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT-4, etc</td>\n",
       "      <td>3.694612</td>\n",
       "      <td>3.694612</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Emotional intelligence</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>3.614010</td>\n",
       "      <td>3.614010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Finance</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>davinci</td>\n",
       "      <td>3.468992</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>General knowledge, IQ (Binet-Simon Scale, verb...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bing Chat</td>\n",
       "      <td>3.385513</td>\n",
       "      <td>3.703680</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Japan: National Medical Licensure Examination,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GPT-4.5</td>\n",
       "      <td>3.355234</td>\n",
       "      <td>3.355234</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Being human</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>2.381280</td>\n",
       "      <td>4.168894</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>Academia, Aerospace, Art (via prompting Midjou...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  quality_score_mean  quality_score_max  achievements_count  \\\n",
       "8       Gemini 3            4.476880           4.476880                   1   \n",
       "3    Claude 3.6S            4.415792           4.415792                   1   \n",
       "14  o3-mini-high            4.360643           4.360643                   1   \n",
       "13            o1            4.040036           4.500000                   2   \n",
       "5     GPT-4, etc            3.694612           3.694612                   1   \n",
       "15       o4-mini            3.614010           3.614010                   1   \n",
       "10       davinci            3.468992           4.000000                   4   \n",
       "0      Bing Chat            3.385513           3.703680                   2   \n",
       "6        GPT-4.5            3.355234           3.355234                   1   \n",
       "4          GPT-4            2.381280           4.168894                  16   \n",
       "\n",
       "    domains_count  peer_reviewed_rate  outperforms_rate  \\\n",
       "8               1                 0.0            1.0000   \n",
       "3               1                 0.0            1.0000   \n",
       "14              1                 0.0            1.0000   \n",
       "13              2                 0.0            1.0000   \n",
       "5               1                 0.0            1.0000   \n",
       "15              1                 0.0            1.0000   \n",
       "10              3                 0.0            1.0000   \n",
       "0               2                 0.0            1.0000   \n",
       "6               1                 0.0            1.0000   \n",
       "4              15                 0.0            0.9375   \n",
       "\n",
       "                                      domains_covered  quality_tier  \n",
       "8                                       Transcription             5  \n",
       "3                                          Persuasion             5  \n",
       "14                                     Health reviews             5  \n",
       "13                                    Maths, Medicine             5  \n",
       "5                              Emotional intelligence             4  \n",
       "15                                            Finance             4  \n",
       "10  General knowledge, IQ (Binet-Simon Scale, verb...             4  \n",
       "0   Japan: National Medical Licensure Examination,...             3  \n",
       "6                                         Being human             3  \n",
       "4   Academia, Aerospace, Art (via prompting Midjou...             3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: model_profiles.csv\n",
      "upload: ./model_profiles.csv to s3://sagemaker-us-east-1-907086662522/processed/model_profiles.csv\n",
      "2026-02-22 23:20:31       1769 model_profiles.csv\n",
      "2026-02-22 22:40:31    2667107 synthetic_requests_labeled_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Aggregate to model-level profiles ---\n",
    "# We keep: avg score, count of achievements, domains covered, peer-reviewed ratio\n",
    "agg = df.groupby(\"Model\", dropna=False).agg(\n",
    "    quality_score_mean=(\"quality_score\", \"mean\"),\n",
    "    quality_score_max=(\"quality_score\", \"max\"),\n",
    "    achievements_count=(\"Achievement\", \"count\"),\n",
    "    domains_count=(\"Field\", lambda x: x.nunique()),\n",
    "    peer_reviewed_rate=(\"peer_reviewed_flag\", \"mean\"),\n",
    "    outperforms_rate=(\"outperforms_flag\", \"mean\"),\n",
    ").reset_index()\n",
    "\n",
    "# domains list\n",
    "domains = df.groupby(\"Model\")[\"Field\"].apply(lambda s: \", \".join(sorted(set([str(x).strip() for x in s.dropna()])))).reset_index()\n",
    "domains = domains.rename(columns={\"Field\": \"domains_covered\"})\n",
    "model_profiles = agg.merge(domains, on=\"Model\", how=\"left\")\n",
    "\n",
    "# --- Create tiers (1..5) for routing constraints ---\n",
    "# Use quantiles so tiers are balanced\n",
    "model_profiles[\"quality_tier\"] = pd.qcut(\n",
    "    model_profiles[\"quality_score_mean\"].rank(method=\"first\"),\n",
    "    q=min(5, model_profiles.shape[0]),\n",
    "    labels=False\n",
    ") + 1\n",
    "\n",
    "# sort helpful\n",
    "model_profiles = model_profiles.sort_values([\"quality_tier\", \"quality_score_mean\"], ascending=[False, False])\n",
    "\n",
    "print(\"Model profiles preview:\")\n",
    "display(model_profiles.head(10))\n",
    "\n",
    "# --- Save locally and upload to S3 ---\n",
    "model_profiles.to_csv(out_local, index=False)\n",
    "print(\"Saved:\", out_local)\n",
    "\n",
    "# upload (works in SageMaker notebooks)\n",
    "!aws s3 cp model_profiles.csv {out_s3}\n",
    "!aws s3 ls s3://{bucket}/processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d048e-9c2e-46f1-8902-890a43551984",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebook generated aggregated model capability profiles from benchmark performance data.\n",
    "\n",
    "The resulting dataset provides structured inputs for model routing, comparison, and cost-performance optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
